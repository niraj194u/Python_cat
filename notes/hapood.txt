HADOOP HDFS
--------------------------------------------------------------------------------------------
13.40.76.180

user:ec2-user

ssh -i "test_key.pem" ec2-user@13.40.76.180

ssh -i "test_key.pem" ec2-user@18.134.132.202

icacls "D:\AWS\test_key.pem" /inheritance:r /grant:r "${env:USERNAME}:(R)"

------------------------------------------------------------------------------------------------


hadoop jar /opt/cloudera/parcels/CDH/lib/hadoop-mapreduce/hadoop-streaming.jar \
 -files mapper.py,reducer.py \
 -mapper "/usr/bin/python3 mapper.py" \
 -reducer "/usr/bin/python3 reducer.py" \
 -input /tmp/bigdata_nov_2024/niraj/hello.txt \
 -output /tmp/bigdata_nov_2024/niraj/output2
--------------------------------------------------------------------------------------

hadoop jar /opt/cloudera/parcels/CDH/lib/hadoop-mapreduce/hadoop-streaming.jar \
 -files mapper1.py,reducer1.py \
 -mapper "/usr/bin/python3 mapper1.py" \
 -reducer "/usr/bin/python3 reducer1.py" \
 -input /tmp/bigdata_nov_2024/niraj/data1.csv \
 -output /tmp/bigdata_nov_2024/niraj/output12
-------------------------------------------------------------------------------------


cat bdata.csv | python3 mapper1.py | sort | python3 reducer1.py

-------------------------------------------------------------------------------
sudo -u hdfs hadoop jar /opt/cloudera/parcels/CDH/lib/hadoop-mapreduce/hadoop-mapreduce-examples.jar wordcount /tmp/bigdata_nov_2024/obinna/hello.txt /tmp/bigdata_nov_2024/obinna/count1

hdfs dfs -ls -R: Tree command in linux

hdfs dfs -ls /tmp/bigdata_nov_2024/

hdfs dfs -rm -r /tmp/bigdata_nov_2024/niraj/hello.txt

hdfs dfs -mkdir /tmp/bigdata_nov_2024/{your name}

scp -i .\test_key.pem .\data.csv ec2-user@13.40.76.180:/home/ec2-user/bigdata_nov_2024/niraj/ :to copy file from local to ec2 instance
scp -i .\test_key.pem .\data.csv ec2-user@18.134.132.202:/home/ec2-user/bigdata_nov_2024/niraj/

cat ../data.csv | python3 mapper1.py | sort | python3 reducer1.py : To run mapreducer on local linux machine

hdfs dfs -put file.txt /tmp/bigdata_nov_2024/niraj: copy from Linux to hdfs: to copy file from linux machine to Hadoop

----------------------------------------------------------------------------------------------------------------------------------
MAPPER >> Word count

#!/usr/bin/env python
import sys

# Mapper function to read input lines, split into words, and emit word counts
def mapper():
    for line in sys.stdin:
        # Strip leading/trailing spaces and split the line into words
        line = line.strip()
        words = line.split()
        
        for word in words:
            # Output the word with count 1
            print(f"{word.lower()}\t1")

if _name_ == "__main__":
    mapper()
-----------------------------------------------------------------------------------------------------------------------------------------------
REDUCER >> Word count

#!/usr/bin/env python
import sys

# Reducer function to aggregate word counts
def reducer():
    current_word = None
    current_count = 0

    for line in sys.stdin:
        # Strip leading/trailing spaces and split the line into word and count
        line = line.strip()
        word, count = line.split('\t', 1)

        try:
            count = int(count)
        except ValueError:
            # Ignore lines that can't be converted to integers
            continue

        if current_word == word:
            current_count += count
        else:
            if current_word:
                # Output the previous word and its total count
                print(f"{current_word}\t{current_count}")
            current_word = word
            current_count = count

    # Output the last word and its total count
    if current_word == word:
        print(f"{current_word}\t{current_count}")

if _name_ == "__main__":
    reducer()
-----------------------------------------------------------------------------------------------------------------------------------------------------
chmod +x mapper.py
chmod +x reducer.py
----------------------------------------------------------------------------------------------------------------------------------------------------
MAPPER1 >> CSV

#!/usr/bin/env python
import sys
import csv

# Mapper function for CSV processing
def mapper():
    reader = csv.reader(sys.stdin)
    # Skip the header row
    next(reader)

    for row in reader:
        if len(row) < 4:
            continue  # Skip malformed rows

        name = row[1].strip()  # Name column
        category = row[2].strip()  # Category column
        amount = row[3].strip()  # Amount column

        try:
            amount = float(amount)
        except ValueError:
            continue  # Skip rows where amount is not a valid number
        
        # Emitting name or category as key, and amount as value
        # You can change 'name' or 'category' as needed for your use case
        print(f"{category}\t{amount}")

if _name_ == "__main__":
    mapper()
-----------------------------------------------------------------------------------------------------------------------------------------------------

REDUCER1 >> CSV
#!/usr/bin/env python
import sys

# Reducer function for CSV processing (aggregation)
def reducer():
    current_key = None
    current_sum = 0.0

    for line in sys.stdin:
        line = line.strip()
        key, value = line.split('\t', 1)

        try:
            value = float(value)
        except ValueError:
            continue  # Skip lines where value can't be converted to float

        if current_key == key:
            current_sum += value
        else:
            if current_key:
                # Emit the previous key and its summed value
                print(f"{current_key}\t{current_sum}")
            current_key = key
            current_sum = value

    # Emit the last key and its summed value
    if current_key == key:
        print(f"{current_key}\t{current_sum}")

if _name_ == "__main__":
    reducer()
---------------------------------------------------------------------------------------------------------------------------------------------
chmod +x mapper1.py
chmod +x reducer1.py
---------------------------------------------------------------------------------------------------------------------------------------------
